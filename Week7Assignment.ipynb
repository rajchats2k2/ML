{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week7Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajchats2k2/ML/blob/master/Week7Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x6hzh9BwJLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUdNcPmwwnAX",
        "colab_type": "code",
        "outputId": "149661cb-5714-4898-971d-7ad29d00e0df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAmo1q5Uwnqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qy0pTnQPYuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3473db9a-ad7a-4279-d7d7-6714e88a0304"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqOXT0Tpwypu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d5c72db-02f8-4823-a36c-7ab6655310fd"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(len(vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjPVVI_TwWYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCEzmsNGQAJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a9bb8da3-0532-4644-d03a-29dd736dec44"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7-SzwHQDhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07522190-e7b4-43b0-e06b-24832d9a8058"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uGDoIHEwa1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ba863e10-c883-492f-d831-7ee92e6b0b50"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmeRm4Ipw8Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c58005c2-f842-4efa-cadd-02a69504b006"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRg_r3GAxCqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84QMP1PrRKGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e2680541-50d1-4d04-c56f-2a3531478bdf"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXAHItRIxH3o",
        "colab_type": "code",
        "outputId": "44747d29-1ca3-468c-d8f0-60c240a6adc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV9dJ6BYxPsX",
        "colab_type": "text"
      },
      "source": [
        "Use tf.keras.Sequential to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "tf.keras.layers.Embedding (dim=256)\n",
        "tf.keras.layers.LSTM (units=1024)\n",
        "tf.keras.layers.Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrOMzqd4R1Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "lstm_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaIn-h9AxOFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units,\n",
        "                        #return_sequences=True,\n",
        "                        #stateful=True,\n",
        "                        #recurrent_initializer='glorot_uniform')),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKlmDvySSmC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  lstm_units=lstm_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnMSKkz7TlMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f62b75f5-11b2-48f9-cfb5-6a4a224fb3f0"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CofZqamc0M1d",
        "colab_type": "code",
        "outputId": "95d4a734-1951-4727-cb10-4bf598bfefeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 64)            65600     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (64, None, 65)            4225      \n",
            "=================================================================\n",
            "Total params: 5,333,441\n",
            "Trainable params: 5,333,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLz-X-2f0mF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9jsw3FoT1du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f9ff7ec3-e0f4-4d64-fabf-cf219993a618"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 41, 53, 28, 44, 50, 41, 32, 25,  1, 13,  0, 12, 46, 27, 58, 47,\n",
              "       64, 30, 58, 48, 59, 21, 28, 19,  3, 50, 46, 27, 16, 44, 32, 16, 33,\n",
              "       53,  6, 64, 26,  0, 47,  9, 33, 54, 18, 10, 36, 64, 40, 39, 10, 43,\n",
              "       30, 26, 27, 55, 53,  2,  8, 58, 55,  6, 41, 56, 19, 42, 19, 11,  6,\n",
              "       25, 30, 40, 54, 40, 21, 59, 25, 17, 38, 63, 23, 32, 11, 16,  7, 62,\n",
              "        1, 26, 61, 42, 40, 59,  2, 51, 31, 35,  5, 58, 48, 10, 61])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA_TxSdEUFI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6f28ab0c-8e04-4c50-aa66-7c4c6917bb9b"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'nd so are like to do.\\n\\nCOMINIUS:\\nThat is the way to lay the city flat;\\nTo bring the roof to the foun'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"ecoPflcTM A\\n?hOtizRtjuIPG$lhODfTDUo,zN\\ni3UpF:Xzba:eRNOqo!.tq,crGdG;,MRbpbIuMEZyKT;D-x Nwdbu!mSW'tj:w\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_zwIWzz0STu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a74bd131-139b-478b-87b2-2c9b4fab8784"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.173914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEOaIZWjUnX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47yHTPNLU3aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgkoVU5C02Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnOTpRLQ1DZo",
        "colab_type": "code",
        "outputId": "9e1c2188-8589-4ae2-a092-4d7f80f388bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 31s 179ms/step - loss: 2.8628\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 29s 170ms/step - loss: 2.1362\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 29s 171ms/step - loss: 1.8581\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 30s 172ms/step - loss: 1.6845\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 29s 171ms/step - loss: 1.5767\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 29s 171ms/step - loss: 1.5042\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 29s 171ms/step - loss: 1.4521\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 29s 170ms/step - loss: 1.4136\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 29s 170ms/step - loss: 1.3810\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 29s 171ms/step - loss: 1.3547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLMOraf-VLUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d27a892d-e208-4dcb-d86d-55680a5faff1"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdsJIP8hVdH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, lstm_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDQ3-nwGVhqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1376c9e2-37e8-493c-86de-eb9210fb2878"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nel2SQqZVmeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkHJmcffVsty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "68e053dc-3d0f-43eb-d959-933473041925"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: th shall sole life\n",
            "Are you calling you to saily nor condemn'd\n",
            "Like and the haterful man a trait i' the way\n",
            "To most put forth their flain and spots,\n",
            "I'll speak, lest flouts, that Richmond is it were most\n",
            "seeming, that is bother, then at last, thougarts the rock.\n",
            "\n",
            "CAPULET:\n",
            "Reconditual members.\n",
            "\n",
            "ARCELIS:\n",
            "Let's fear:\n",
            "Our debotced men?\n",
            "\n",
            "MAny yields; commend\n",
            "overshis a milk, I'll give you admits might lips\n",
            "To use the hand that now old Oxford is\n",
            "rebuke a servant better.\n",
            "\n",
            "AUFIDIUS:\n",
            "I would, I fly the Lady Grey.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Gran, that is here hath some touches as this:\n",
            "For my rut-le, if we banish you?\n",
            "\n",
            "Citizens:\n",
            "I wishem too, but what hands whose louds hath seed\n",
            "That lands ather out to charm and pull the\n",
            "rust we give you whats, I here provided all the envy:\n",
            "\n",
            "G HENRY VI\n",
            "Come away or, hoor.\n",
            "\n",
            "BAPTISTA:\n",
            "Why, come, here, my master, nor I my time?\n",
            "Tell me thy mother flattering both nexts in me have\n",
            "Divided to content thee fuction from his beh,\n",
            "And give the man: all Romeo is no sacred greatned fo\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}